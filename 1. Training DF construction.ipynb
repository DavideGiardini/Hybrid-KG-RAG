{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from neo4j import GraphDatabase\n",
    "from utils.GraphTraverser import CypherSearch\n",
    "import config.EnvLoader as el\n",
    "import pickle, json\n",
    "\n",
    "URI = \"neo4j://localhost\"\n",
    "AUTH = (\"neo4j\", el.NEO4J_PWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure Embeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    api_key=el.OPENAI_API_KEY,\n",
    "    azure_endpoint=el.AZURE_ENDPOINT,\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    ")\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-4o-global\",\n",
    "    azure_deployment=\"gpt-4o-global\",\n",
    "    api_key=el.OPENAI_API_KEY,\n",
    "    azure_endpoint=el.OPENAI_API_KEY,\n",
    "    openai_api_version=\"2024-02-15-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate this file run Indexing.ipynb\n",
    "with open('embedded_questions.pkl', 'rb') as file: \n",
    "\tq_diz = pickle.load(file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the dataframe\n",
    "\n",
    "The training dataframe consists of 5223 observations: one for each question.\n",
    "For each of the questions, it retains:\n",
    "- The text of the question.\n",
    "- The embedding of the question.\n",
    "- The \"observations\", i.e. all the chunks that have been retrieved by at least one of the retrieval methods.<br>\n",
    "The observation variable will be a python dictionary that contains:\n",
    "- As **keys**: the ID of one chunk that have been retrieved by at least one of the retrieval methods.\n",
    "- As **values**: a dictionary containing, for each of the retrieval methods, the value 1 if they have been retrieved by that method or 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training dataframe\n",
    "df = []\n",
    "for question, embedding in q_diz.items():\n",
    "    diz = {}\n",
    "    diz[\"question\"] = question\n",
    "    diz[\"embedding\"] = embedding\n",
    "    diz[\"observations\"] = {}\n",
    "    df.append(diz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5223"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate the Training Dataframe\n",
    "\n",
    "## Path Retrieval\n",
    "Is the chunk linked to the relevant KG path?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5223/5223 [2:00:32<00:00,  1.38s/it]  \n"
     ]
    }
   ],
   "source": [
    "for el in tqdm(df):\n",
    "    selected_chunks, cost, answer, subC_list = CypherSearch(el[\"embedding\"], el[\"question\"], llm)\n",
    "    el[\"cost\"] = cost\n",
    "    for chunk_id in selected_chunks:\n",
    "        el[\"observations\"][chunk_id] = {\n",
    "            \"graph_traverser\": 1\n",
    "        }\n",
    "    el[\"graph_traverser_output\"] = {\n",
    "        \"answer\": answer,\n",
    "        \"subC_list\": subC_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset.json', 'w') as f:\n",
    "    json.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Retrieval\n",
    "How relevant is the chunk’s content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5223/5223 [02:53<00:00, 30.12it/s]\n"
     ]
    }
   ],
   "source": [
    "for el in tqdm(df):\n",
    "    # Search for top 10 chunk\n",
    "    with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "        retrieved_chunks, _, _ = driver.execute_query(\n",
    "            \"\"\"CALL db.index.vector.queryNodes(\"vector\", 100, $embedding)\n",
    "                YIELD node, score\n",
    "                RETURN ID(node), score\n",
    "                LIMIT 10\"\"\",\n",
    "            embedding=el[\"embedding\"]\n",
    "        )\n",
    "    for chunk in retrieved_chunks:\n",
    "        if chunk[\"ID(node)\"] not in el[\"observations\"].keys():\n",
    "            el[\"observations\"][chunk[\"ID(node)\"]] = {}\n",
    "        el[\"observations\"][chunk[\"ID(node)\"]][\"similarity_score\"] = float(chunk[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page and Section Retrieval\n",
    "How relevant is the document/section to which the chunk belongs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OriginalPage index\n",
    "# Create nodes vector index for vector RAG\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    result, summary, keys = driver.execute_query(\n",
    "        \"\"\"CREATE VECTOR INDEX original_page_vector IF NOT EXISTS\n",
    "        FOR (a:OriginalPage) ON (a.embedding)\n",
    "        OPTIONS {indexConfig: {\n",
    "        `vector.dimensions`: 1536,\n",
    "        `vector.similarity_function`: 'cosine'\n",
    "        }}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OriginalPage index\n",
    "# Create nodes vector index for vector RAG\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    result, summary, keys = driver.execute_query(\n",
    "        \"\"\"CALL apoc.periodic.commit(\"\n",
    "            MATCH (a:OriginalPage)\n",
    "            WHERE NOT a:OpSubc\n",
    "            WITH a LIMIT 1000\n",
    "            SET a :OpSubc\n",
    "            RETURN count(a)\n",
    "        \")\"\"\"\n",
    "    )\n",
    "\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    result, summary, keys = driver.execute_query(\n",
    "        \"\"\"CALL apoc.periodic.commit(\"\n",
    "            MATCH (a:SubChapter)\n",
    "            WHERE NOT a:OpSubc\n",
    "            WITH a LIMIT 1000\n",
    "            SET a :OpSubc\n",
    "            RETURN count(a)\n",
    "        \")\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OpSubc index\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    result, summary, keys = driver.execute_query(\n",
    "        \"\"\"CREATE VECTOR INDEX op_subc_vector IF NOT EXISTS\n",
    "        FOR (a:OpSubc) ON (a.embedding)\n",
    "        OPTIONS {indexConfig: {\n",
    "        `vector.dimensions`: 1536,\n",
    "        `vector.similarity_function`: 'cosine'\n",
    "        }}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5223/5223 [11:36<00:00,  7.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for el in tqdm(df):\n",
    "    # Search for top 3 pages\n",
    "    with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "        retrieved_OP, _, _ = driver.execute_query(\n",
    "            \"\"\"CALL db.index.vector.queryNodes(\"original_page_vector\", 100, $embedding)\n",
    "            YIELD node, score\n",
    "            RETURN ID(node), score\n",
    "            LIMIT 3\"\"\",\n",
    "            embedding=el[\"embedding\"]\n",
    "        )\n",
    "    for OriginalPage in retrieved_OP:\n",
    "        with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "            connected_chunks, _, _ = driver.execute_query(\n",
    "                \"\"\"MATCH (a:OriginalPage)<-[:is_chunk_of]-(b:Chunk)\n",
    "                WHERE ID(a) = $id_a\n",
    "                RETURN ID(b)\"\"\",\n",
    "                id_a = OriginalPage[\"ID(node)\"]\n",
    "            )\n",
    "        for chunk in connected_chunks:\n",
    "            if chunk[\"ID(b)\"] not in el[\"observations\"].keys():\n",
    "                el[\"observations\"][chunk[\"ID(b)\"]] = {}\n",
    "            el[\"observations\"][chunk[\"ID(b)\"]][\"page_similarity_score\"] = float(OriginalPage[\"score\"])\n",
    "    # Search for top 5 SubChapters\n",
    "    with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "        retrieved_subC, _, _ = driver.execute_query(\n",
    "            \"\"\"CALL db.index.vector.queryNodes(\"op_subc_vector\", 100, $embedding)\n",
    "            YIELD node, score\n",
    "            WHERE node:OriginalPage OR node:SubChapter\n",
    "            RETURN ID(node), score\n",
    "            LIMIT 5\"\"\",\n",
    "            embedding=el[\"embedding\"]\n",
    "        )\n",
    "    for subC in retrieved_subC:\n",
    "        with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "            connected_chunks, _, _ = driver.execute_query(\n",
    "                \"\"\"MATCH (a)-[:has_chunk]->(b:Chunk)\n",
    "                WHERE ID(a) = $id_a\n",
    "                RETURN ID(b)\"\"\",\n",
    "                id_a = subC[\"ID(node)\"]\n",
    "            )\n",
    "        for chunk in connected_chunks:\n",
    "            if chunk[\"ID(b)\"] not in el[\"observations\"].keys():\n",
    "                el[\"observations\"][chunk[\"ID(b)\"]] = {}\n",
    "            el[\"observations\"][chunk[\"ID(b)\"]][\"parent_similarity_score\"] = float(subC[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Retrieval\n",
    "How relevant are the entities that the chunk cites?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OpSubc index\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    result, summary, keys = driver.execute_query(\n",
    "        \"\"\"CREATE VECTOR INDEX page_vector IF NOT EXISTS\n",
    "        FOR (a:Page) ON (a.embedding)\n",
    "        OPTIONS {indexConfig: {\n",
    "        `vector.dimensions`: 1536,\n",
    "        `vector.similarity_function`: 'cosine'\n",
    "        }}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5223/5223 [07:33<00:00, 11.52it/s]\n"
     ]
    }
   ],
   "source": [
    "for el in tqdm(df):\n",
    "    # Search for top 5 entities\n",
    "    with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "        retrieved_entities, _, _ = driver.execute_query(\n",
    "            \"\"\"CALL db.index.vector.queryNodes(\"page_vector\", 100, $embedding)\n",
    "            YIELD node, score\n",
    "            RETURN ID(node), score\n",
    "            LIMIT 5\"\"\",\n",
    "            embedding=el[\"embedding\"]\n",
    "        )\n",
    "    for entity in retrieved_entities:\n",
    "        with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "            connected_chunks, _, _ = driver.execute_query(\n",
    "                \"\"\"MATCH (a:Page)-[:cited_in]->(b:Chunk)\n",
    "                WHERE ID(a) = $id_a\n",
    "                RETURN ID(b)\"\"\",\n",
    "                id_a = entity[\"ID(node)\"]\n",
    "            )\n",
    "        for chunk in connected_chunks:\n",
    "            if chunk[\"ID(b)\"] not in el[\"observations\"].keys():\n",
    "                el[\"observations\"][chunk[\"ID(b)\"]] = {}\n",
    "            entity_similarity_score = el[\"observations\"][chunk[\"ID(b)\"]].get(\"entity_similarity_score\", 0)\n",
    "            el[\"observations\"][chunk[\"ID(b)\"]][\"entity_similarity_score\"] = max(float(entity[\"score\"]), entity_similarity_score)\n",
    "            n_relevant_entities = el[\"observations\"][chunk[\"ID(b)\"]].get(\"n_relevant_entities\", 0)\n",
    "            el[\"observations\"][chunk[\"ID(b)\"]][\"n_relevant_entities\"] = n_relevant_entities + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth\n",
    "Does the chunk contain the actual answer to the question? (Ground truth from the NQ dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5223/5223 [01:55<00:00, 45.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for el in tqdm(df):\n",
    "    # Search for top 3 pages\n",
    "    with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "        answers, _, _ = driver.execute_query(\n",
    "            \"\"\"MATCH (a:Chunk {is_answer_of: $question})\n",
    "            RETURN ID(a)\"\"\",\n",
    "            question=el[\"question\"]\n",
    "        )\n",
    "    for chunk in answers:\n",
    "        if chunk[\"ID(a)\"] not in el[\"observations\"].keys():\n",
    "            el[\"observations\"][chunk[\"ID(a)\"]] = {}\n",
    "        el[\"observations\"][chunk[\"ID(a)\"]][\"is_answer\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset.json', 'w') as f:\n",
    "    json.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset exploration\n",
    "This section of the notebook was used to compute the metrics for each of the individual retrieval systems (section 4.3.2 of Master Thesis).\n",
    "\n",
    "- **Precision**: how many of the selected chunks contain the answer?\n",
    "- **Recall**: what percentage of chunks containing the answer have been found?\n",
    "- **Detection**: (Bool) does at least one of the collected chunks contain the answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('Dataset.json', 'r') as file:\n",
    "    df = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_score(item):\n",
    "    return item.get('similarity_score', 0)\n",
    "\n",
    "# Precision, recall and detection for top 3, top 5 and top 10 relevant chunks\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "detection_lst = []\n",
    "\n",
    "for question in df:\n",
    "    sorted_lst = sorted(question[\"observations\"].values(), key=get_similarity_score, reverse=True)[:10]\n",
    "    precision_diz = {}\n",
    "    recall_diz = {}\n",
    "    detection_diz = {}\n",
    "    for n in [3,5,10]:\n",
    "        chunk_lst = sorted_lst[:n]\n",
    "        total_answer = 0\n",
    "        answers_found = sum([chunk.get(\"is_answer\", 0) for chunk in chunk_lst])\n",
    "        total_answer = sum([chunk.get(\"is_answer\", 0) for chunk in question[\"observations\"].values()])\n",
    "        precision_diz[n] = answers_found/n\n",
    "        recall_diz[n] = answers_found/total_answer\n",
    "        detection_diz[n] = min(answers_found, 1)\n",
    "    precision_lst.append(precision_diz)\n",
    "    recall_lst.append(recall_diz)\n",
    "    detection_lst.append(detection_diz)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 3 CHUNKS\n",
      "Average Precision: 0.237\n",
      "Average Recall: 0.528\n",
      "Average Detection: 0.617\n",
      "TOP 5 CHUNKS\n",
      "Average Precision: 0.172\n",
      "Average Recall: 0.626\n",
      "Average Detection: 0.708\n",
      "TOP 10 CHUNKS\n",
      "Average Precision: 0.103\n",
      "Average Recall: 0.739\n",
      "Average Detection: 0.801\n"
     ]
    }
   ],
   "source": [
    "length = len(df)\n",
    "\n",
    "for n in [3,5,10]:\n",
    "    print(f\"TOP {n} CHUNKS\")\n",
    "    print(f\"Average Precision: {round(sum([x[n] for x in precision_lst])/length,3)}\")\n",
    "    print(f\"Average Recall: {round(sum([x[n] for x in recall_lst])/length,3)}\")\n",
    "    print(f\"Average Detection: {round(sum([x[n] for x in detection_lst])/length,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall and detection for top 1, top 2 and top 3 relevant pages\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "detection_lst = []\n",
    "\n",
    "for question in df:\n",
    "    pages_similarity_scores = list(set([x.get('page_similarity_score', 0) for x in question[\"observations\"].values()]))\n",
    "    pages_similarity_scores.sort(reverse=True)\n",
    "    precision_diz = {}\n",
    "    recall_diz = {}\n",
    "    detection_diz = {}\n",
    "    for n in [1,2,3]:\n",
    "        similarity_lst = pages_similarity_scores[:n]\n",
    "        chunk_lst = [x for x in question[\"observations\"].values() if x.get('page_similarity_score', 0) in similarity_lst]\n",
    "        total_answer = 0\n",
    "        answers_found = sum([chunk.get(\"is_answer\", 0) for chunk in chunk_lst])\n",
    "        total_answer = sum([chunk.get(\"is_answer\", 0) for chunk in question[\"observations\"].values()])\n",
    "        precision_diz[n] = answers_found/len(chunk_lst)\n",
    "        recall_diz[n] = answers_found/total_answer\n",
    "        detection_diz[n] = min(answers_found, 1)\n",
    "    precision_lst.append(precision_diz)\n",
    "    recall_lst.append(recall_diz)\n",
    "    detection_lst.append(detection_diz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 1 Relevant Pages\n",
      "Average Precision: 0.071\n",
      "Average Recall: 0.789\n",
      "Average Detection: 0.789\n",
      "TOP 2 Relevant Pages\n",
      "Average Precision: 0.029\n",
      "Average Recall: 0.861\n",
      "Average Detection: 0.861\n",
      "TOP 3 Relevant Pages\n",
      "Average Precision: 0.018\n",
      "Average Recall: 0.885\n",
      "Average Detection: 0.885\n"
     ]
    }
   ],
   "source": [
    "length = len(df)\n",
    "\n",
    "for n in [1,2,3]:\n",
    "    print(f\"TOP {n} Relevant Pages\")\n",
    "    print(f\"Average Precision: {round(sum([x[n] for x in precision_lst])/length,3)}\")\n",
    "    print(f\"Average Recall: {round(sum([x[n] for x in recall_lst])/length,3)}\")\n",
    "    print(f\"Average Detection: {round(sum([x[n] for x in detection_lst])/length,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall and detection for top 1,2,3,4,5 relevant sections.\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "detection_lst = []\n",
    "\n",
    "for question in df:\n",
    "    pages_similarity_scores = list(set([x.get('parent_similarity_score', 0) for x in question[\"observations\"].values()]))\n",
    "    pages_similarity_scores.sort(reverse=True)\n",
    "    precision_diz = {}\n",
    "    recall_diz = {}\n",
    "    detection_diz = {}\n",
    "    for n in [1,2,3,4,5]:\n",
    "        similarity_lst = pages_similarity_scores[:n]\n",
    "        chunk_lst = [x for x in question[\"observations\"].values() if x.get('parent_similarity_score', 0) in similarity_lst]\n",
    "        total_answer = 0\n",
    "        answers_found = sum([chunk.get(\"is_answer\", 0) for chunk in chunk_lst])\n",
    "        total_answer = sum([chunk.get(\"is_answer\", 0) for chunk in question[\"observations\"].values()])\n",
    "        precision_diz[n] = answers_found/len(chunk_lst)\n",
    "        recall_diz[n] = answers_found/total_answer\n",
    "        detection_diz[n] = min(answers_found, 1)\n",
    "    precision_lst.append(precision_diz)\n",
    "    recall_lst.append(recall_diz)\n",
    "    detection_lst.append(detection_diz)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 1 Relevant Parents\n",
      "Average Precision: 0.207\n",
      "Average Recall: 0.352\n",
      "Average Detection: 0.421\n",
      "TOP 2 Relevant Parents\n",
      "Average Precision: 0.144\n",
      "Average Recall: 0.513\n",
      "Average Detection: 0.579\n",
      "TOP 3 Relevant Parents\n",
      "Average Precision: 0.113\n",
      "Average Recall: 0.604\n",
      "Average Detection: 0.663\n",
      "TOP 4 Relevant Parents\n",
      "Average Precision: 0.088\n",
      "Average Recall: 0.676\n",
      "Average Detection: 0.727\n",
      "TOP 5 Relevant Parents\n",
      "Average Precision: 0.058\n",
      "Average Recall: 0.798\n",
      "Average Detection: 0.829\n"
     ]
    }
   ],
   "source": [
    "length = len(df)\n",
    "\n",
    "for n in [1,2,3,4,5]:\n",
    "    print(f\"TOP {n} Relevant Parents\")\n",
    "    print(f\"Average Precision: {round(sum([x[n] for x in precision_lst])/length,3)}\")\n",
    "    print(f\"Average Recall: {round(sum([x[n] for x in recall_lst])/length,3)}\")\n",
    "    print(f\"Average Detection: {round(sum([x[n] for x in detection_lst])/length,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "839\n"
     ]
    }
   ],
   "source": [
    "# Precision, recall and detection for top 1,2,3,4,5 relevant entities.\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "detection_lst = []\n",
    "\n",
    "i = 0\n",
    "for question in df:\n",
    "    pages_similarity_scores = list(set([x.get('entity_similarity_score', 0) for x in question[\"observations\"].values()]))\n",
    "    pages_similarity_scores.sort(reverse=True)\n",
    "    pages_similarity_scores = pages_similarity_scores[:-1]\n",
    "    precision_diz = {}\n",
    "    recall_diz = {}\n",
    "    detection_diz = {}\n",
    "    if not pages_similarity_scores:\n",
    "        print(i)\n",
    "        continue\n",
    "    for n in [1,2,3,4,5]:\n",
    "        similarity_lst = pages_similarity_scores[:n]\n",
    "        chunk_lst = [x for x in question[\"observations\"].values() if x.get('entity_similarity_score', 0) in similarity_lst]\n",
    "        total_answer = 0\n",
    "        answers_found = sum([chunk.get(\"is_answer\", 0) for chunk in chunk_lst])\n",
    "        total_answer = sum([chunk.get(\"is_answer\", 0) for chunk in question[\"observations\"].values()])\n",
    "        if len(chunk_lst) == 0:\n",
    "            print(i)\n",
    "        precision_diz[n] = answers_found/len(chunk_lst)\n",
    "        recall_diz[n] = answers_found/total_answer\n",
    "        detection_diz[n] = min(answers_found, 1)\n",
    "    precision_lst.append(precision_diz)\n",
    "    recall_lst.append(recall_diz)\n",
    "    detection_lst.append(detection_diz)\n",
    "    i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 1 Relevant Entities\n",
      "Average Precision: 0.059\n",
      "Average Recall: 0.096\n",
      "Average Detection: 0.118\n",
      "TOP 2 Relevant Entities\n",
      "Average Precision: 0.051\n",
      "Average Recall: 0.161\n",
      "Average Detection: 0.198\n",
      "TOP 3 Relevant Entities\n",
      "Average Precision: 0.046\n",
      "Average Recall: 0.205\n",
      "Average Detection: 0.249\n",
      "TOP 4 Relevant Entities\n",
      "Average Precision: 0.042\n",
      "Average Recall: 0.233\n",
      "Average Detection: 0.281\n",
      "TOP 5 Relevant Entities\n",
      "Average Precision: 0.041\n",
      "Average Recall: 0.248\n",
      "Average Detection: 0.299\n"
     ]
    }
   ],
   "source": [
    "length = len(df)\n",
    "\n",
    "for n in [1,2,3,4,5]:\n",
    "    print(f\"TOP {n} Relevant Entities\")\n",
    "    print(f\"Average Precision: {round(sum([x[n] for x in precision_lst])/length,3)}\")\n",
    "    print(f\"Average Recall: {round(sum([x[n] for x in recall_lst])/length,3)}\")\n",
    "    print(f\"Average Detection: {round(sum([x[n] for x in detection_lst])/length,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Traverser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_lst = []\n",
    "recall_lst = []\n",
    "detection_lst = []\n",
    "\n",
    "minus = 0\n",
    "for question in df:\n",
    "    chunk_lst = [x for x in question[\"observations\"].keys() if \"graph_traverser\" in question[\"observations\"][x].keys()]\n",
    "    if len(chunk_lst) == 0:\n",
    "        minus += 1\n",
    "        continue\n",
    "    answers_found = sum([question[\"observations\"][chunk].get(\"is_answer\", 0) for chunk in chunk_lst])\n",
    "    total_answer = sum([chunk.get(\"is_answer\", 0) for chunk in question[\"observations\"].values()])\n",
    "    precision_lst.append(answers_found/len(chunk_lst))\n",
    "    recall_lst.append(answers_found/total_answer)\n",
    "    detection_lst.append(min(answers_found, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cypher Search\n",
      "Average Precision: 0.266\n",
      "Average Recall: 0.423\n",
      "Average Detection: 0.512\n"
     ]
    }
   ],
   "source": [
    "length = len(df) - minus\n",
    "\n",
    "print(f\"Cypher Search\")\n",
    "print(f\"Average Precision: {round(sum([x for x in precision_lst])/length,3)}\")\n",
    "print(f\"Average Recall: {round(sum([x for x in recall_lst])/length,3)}\")\n",
    "print(f\"Average Detection: {round(sum([x for x in detection_lst])/length,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset cleaning\n",
    "Apply local standardization and handle missing values, as reported in section 3.2.2 of Master Thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('Dataset.json', 'r') as file:\n",
    "    df = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE CONTINUOUS VARIABLES\n",
    "\n",
    "normalized_df = df.copy()\n",
    "\n",
    "for el in normalized_df:\n",
    "    el = el[\"observations\"]\n",
    "    # Prendiamo tutti i similarity score presenti\n",
    "    similarity_lst = []\n",
    "    for value in el.values():\n",
    "        if value.get(\"similarity_score\"):\n",
    "            similarity_lst.append(value[\"similarity_score\"])\n",
    "        if value.get(\"page_similarity_score\"):\n",
    "            similarity_lst.append(value[\"page_similarity_score\"])\n",
    "        if value.get(\"entity_similarity_score\"):\n",
    "            similarity_lst.append(value[\"entity_similarity_score\"])\n",
    "        if value.get(\"parent_similarity_score\"):\n",
    "            similarity_lst.append(value[\"parent_similarity_score\"])\n",
    "    similarity_lst.append(0.75)\n",
    "    similarity_lst = list(set(similarity_lst))\n",
    "    # Compute mean and std_dev\n",
    "    mean = sum(similarity_lst) / len(similarity_lst)\n",
    "    variance = sum([((x - mean) ** 2) for x in similarity_lst]) / len(similarity_lst)\n",
    "    std_dev = variance ** 0.5\n",
    "    # Apply local standardization\n",
    "    for value in el.values():\n",
    "        if value.get(\"similarity_score\"):\n",
    "            value[\"similarity_score\"] = (value[\"similarity_score\"] - mean)/std_dev\n",
    "        else:\n",
    "            value[\"similarity_score\"] = (0.75 - mean)/std_dev\n",
    "        if value.get(\"page_similarity_score\"):\n",
    "            value[\"page_similarity_score\"] = (value[\"page_similarity_score\"] - mean)/std_dev\n",
    "        else:\n",
    "            value[\"page_similarity_score\"] = (0.75 - mean)/std_dev\n",
    "        if value.get(\"entity_similarity_score\"):\n",
    "            value[\"entity_similarity_score\"] = (value[\"entity_similarity_score\"] - mean)/std_dev\n",
    "        else:\n",
    "            value[\"entity_similarity_score\"] = (0.75 - mean)/std_dev\n",
    "        if value.get(\"parent_similarity_score\"):\n",
    "            value[\"parent_similarity_score\"] = (value[\"parent_similarity_score\"] - mean)/std_dev\n",
    "        else:\n",
    "            value[\"parent_similarity_score\"] = (0.75 - mean)/std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESOLVE BINARY/DISCRETE VARIABLES: report 0 when the value is not present\n",
    "for el in normalized_df:\n",
    "    el = el[\"observations\"]\n",
    "    for value in el.values():\n",
    "        if not value.get(\"n_relevant_entities\"):\n",
    "            value[\"n_relevant_entities\"] = 0\n",
    "        if not value.get(\"is_answer\"):\n",
    "            value[\"is_answer\"] = 0\n",
    "        if not value.get(\"graph_traverser\"):\n",
    "            value[\"graph_traverser\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH TRAIN DATA: 4179\n",
      "LENGTH TEST DATA: 1044\n"
     ]
    }
   ],
   "source": [
    "# SPLIT INTO TRAIN AND TEST SET\n",
    "import random, json\n",
    "random.seed(42)\n",
    "split = int(len(normalized_df) * 0.2)\n",
    "\n",
    "random.shuffle(normalized_df)\n",
    "\n",
    "test_data = normalized_df[:split]\n",
    "train_data = normalized_df[split:]\n",
    "\n",
    "print(f\"LENGTH TRAIN DATA: {len(train_data)}\")\n",
    "print(f\"LENGTH TEST DATA: {len(test_data)}\")\n",
    "\n",
    "with open('train_data.json', 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "with open('test_data.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>graph_traverser</th>\n",
       "      <th>page_similarity_score</th>\n",
       "      <th>parent_similarity_score</th>\n",
       "      <th>entity_similarity_score</th>\n",
       "      <th>n_relevant_entities</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>is_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.637866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>0.175379</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.291659</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>1</td>\n",
       "      <td>0.467902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.369943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0.122613</td>\n",
       "      <td>0.175379</td>\n",
       "      <td>1</td>\n",
       "      <td>0.319289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.295008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.063363</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264867</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.070623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.063363</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.063363</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.063363</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0.063363</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0.063363</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0.063363</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.063363</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.420010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    graph_traverser  page_similarity_score  parent_similarity_score  \\\n",
       "0                 1               0.334291                 0.334291   \n",
       "1                 1               0.334291                 0.334291   \n",
       "2                 1               0.334291                 0.334291   \n",
       "3                 1               0.334291                 0.334291   \n",
       "4                 1               0.334291                 0.334291   \n",
       "5                 1               0.334291                 0.334291   \n",
       "6                 0              -4.420010                -4.420010   \n",
       "7                 0              -4.420010                -4.420010   \n",
       "8                 0              -4.420010                 0.122613   \n",
       "9                 0              -4.420010                -4.420010   \n",
       "10                0               0.063363                -4.420010   \n",
       "11                0              -4.420010                -4.420010   \n",
       "12                0              -4.420010                -4.420010   \n",
       "13                0              -4.420010                -4.420010   \n",
       "14                0               0.334291                -4.420010   \n",
       "15                0               0.334291                -4.420010   \n",
       "16                0               0.334291                -4.420010   \n",
       "17                0               0.334291                -4.420010   \n",
       "18                0               0.063363                -4.420010   \n",
       "19                0               0.063363                -4.420010   \n",
       "20                0               0.063363                -4.420010   \n",
       "21                0               0.063363                -4.420010   \n",
       "22                0               0.063363                -4.420010   \n",
       "23                0               0.063363                -4.420010   \n",
       "24                0               0.063363                -4.420010   \n",
       "\n",
       "    entity_similarity_score  n_relevant_entities  similarity_score  is_answer  \n",
       "0                  0.334291                    3         -4.420010          0  \n",
       "1                 -4.420010                    0          0.637866          1  \n",
       "2                  0.175379                    1         -4.420010          0  \n",
       "3                 -4.420010                    0         -4.420010          0  \n",
       "4                 -4.420010                    0          0.291659          0  \n",
       "5                 -4.420010                    0         -4.420010          0  \n",
       "6                  0.334291                    1          0.467902          0  \n",
       "7                 -4.420010                    0          0.369943          0  \n",
       "8                  0.175379                    1          0.319289          0  \n",
       "9                 -4.420010                    0          0.295008          0  \n",
       "10                -4.420010                    0          0.264867          0  \n",
       "11                -4.420010                    0          0.082345          0  \n",
       "12                -4.420010                    0          0.070623          0  \n",
       "13                -4.420010                    0          0.062251          0  \n",
       "14                -4.420010                    0         -4.420010          0  \n",
       "15                -4.420010                    0         -4.420010          0  \n",
       "16                -4.420010                    0         -4.420010          0  \n",
       "17                -4.420010                    0         -4.420010          0  \n",
       "18                -4.420010                    0         -4.420010          0  \n",
       "19                -4.420010                    0         -4.420010          0  \n",
       "20                -4.420010                    0         -4.420010          0  \n",
       "21                -4.420010                    0         -4.420010          0  \n",
       "22                -4.420010                    0         -4.420010          0  \n",
       "23                -4.420010                    0         -4.420010          0  \n",
       "24                -4.420010                    0         -4.420010          0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXPAND THE DATASET\n",
    "# Instead of having one observation for each question, we expand it to have one observation for each chunk\n",
    "tot_lst = []\n",
    "for el in train_data:\n",
    "    tot_lst += list(el[\"observations\"].values())\n",
    "\n",
    "dataframe = pd.DataFrame(tot_lst)\n",
    "dataframe.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGHT OF POSITIVE OBSERVATIONS: 6245\n"
     ]
    }
   ],
   "source": [
    "# Subset the df to balance classes\n",
    "positive_obs = dataframe[dataframe[\"is_answer\"] == 1]\n",
    "num_positive_obs = len(positive_obs)\n",
    "print(f\"LENGHT OF POSITIVE OBSERVATIONS: {num_positive_obs}\")\n",
    "\n",
    "negative_obs = dataframe[dataframe[\"is_answer\"] == 0]\n",
    "negative_obs = negative_obs.sample(num_positive_obs)\n",
    "\n",
    "final_df = pd.concat([positive_obs, negative_obs])\n",
    "# Shuffle\n",
    "final_df = final_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH OF THE FINAL TRAINING DF: 12490\n"
     ]
    }
   ],
   "source": [
    "print(f\"LENGTH OF THE FINAL TRAINING DF: {len(final_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"training_dataframe.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
